{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Dataset/train_preprocessing.csv')\n",
    "test = pd.read_csv('Dataset/test_preprocessing.csv')\n",
    "gender_submission = pd.read_csv('Dataset/gender_submission.csv')\n",
    "# Merge the test dataframe with the gender_submission dataframe on 'PassengerId'\n",
    "test = pd.merge(test, gender_submission[['PassengerId', 'Survived']], on='PassengerId', how='left')\n",
    "test = test.drop(['Survived_x'], axis = 1)\n",
    "test.rename(columns={'Survived_y': 'Survived'}, inplace=True)\n",
    "print(test.head())\n",
    "train = train[train['Survived'] != 'U']\n",
    "test = test[test['Survived'] != 'U']\n",
    "validation, test = train_test_split(test, test_size=2/3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Cabin', 'Embarked', 'Title']].values\n",
    "Y_train = train[['Fare']].values\n",
    "X_train[:,0] = X_train[:,0].astype(float)\n",
    "X_validation = validation[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Cabin', 'Embarked', 'Title']].values\n",
    "Y_validation = validation[['Fare']].values\n",
    "X_validation[:,0] = X_validation[:,0].astype(float)\n",
    "X_test = test[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Cabin', 'Embarked', 'Title']].values\n",
    "Y_test = test[['Fare']].values\n",
    "X_test[:,0] = X_test[:,0].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn model\n",
    "# Modeling\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "print(model.intercept_, model.coef_)\n",
    "# Predict and Test\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "res = pd.DataFrame(data=np.concatenate([Y_pred, Y_test, abs(Y_test - Y_pred)], axis=1),\n",
    "                   columns=[\"Prediction\", \"Ground Truth\", \"Error\"])\n",
    "\n",
    "print(res.head(5))\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(Y_test, Y_pred))\n",
    "print(f'RMSE = {rmse}')\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "print(f'R2 Score = {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from scratch\n",
    "class Linear_Model():\n",
    "    def init(self):\n",
    "        self.coef_ = None\n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        X_train = np.array(X_train,dtype=np.float64)\n",
    "        Y_train = np.array(Y_train,dtype=np.float64)\n",
    "        A = np.concatenate([np.ones(shape=(X_train.shape[0], 1)), X_train], axis = 1)\n",
    "        self.coef_ = (np.matmul(np.matmul(np.linalg.inv(np.matmul(A.T, A)), A.T), Y_train)).T\n",
    "        return\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        A_test = np.concatenate([np.ones((X_test.shape[0], 1)), X_test], axis=1)\n",
    "        Y_pred = np.matmul(A_test, self.coef_.T)\n",
    "        return Y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLS_scratch = Linear_Model()\n",
    "OLS_scratch.fit(X_train, Y_train)\n",
    "print(OLS_scratch.coef_)\n",
    "Y_pred_1s = OLS_scratch.predict(X_test)\n",
    "# Predict and Test\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "res = pd.DataFrame(data=np.concatenate([Y_pred_1s, Y_test, abs(Y_test - Y_pred_1s)], axis=1),\n",
    "                   columns=[\"Prediction\", \"Ground Truth\", \"Error\"])\n",
    "\n",
    "print(res.head(5))\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(Y_test, Y_pred_1s))\n",
    "print(f'RMSE = {rmse}')\n",
    "r2 = r2_score(Y_test, Y_pred_1s)\n",
    "print(f'R2 Score = {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = linear_model.Ridge(alpha=0.1)\n",
    "ridge_model.fit(X_train, Y_train)\n",
    "\n",
    "print(ridge_model.intercept_, ridge_model.coef_)\n",
    "\n",
    "Y_pred_2 = ridge_model.predict(X_test)\n",
    "res = pd.DataFrame(data=np.concatenate([Y_pred_2, Y_test, abs(Y_test - Y_pred_2)], axis=1),\n",
    "                   columns=[\"Prediction\", \"Ground Truth\", \"Error\"])\n",
    "print(res.head(5))\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(Y_test, Y_pred_2))\n",
    "print(f'RMSE = {rmse}')\n",
    "r2 = r2_score(Y_test, Y_pred_2)\n",
    "print(f'R2 Score = {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from scratch\n",
    "class Ridge_Model(Linear_Model):\n",
    "    def __init__(self, alpha=0):\n",
    "        Linear_Model.__init__(self)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        X_train = np.array(X_train,dtype=np.float64)\n",
    "        Y_train = np.array(Y_train,dtype=np.float64)\n",
    "        A = np.concatenate([np.ones(shape=(X_train.shape[0], 1)), X_train], axis = 1)\n",
    "        self.coef_ = (np.matmul(np.matmul(np.linalg.inv(np.matmul(A.T, A) + self.alpha*np.identity(A.shape[1])), A.T), Y_train)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_2 = Ridge_Model(alpha=0.1)\n",
    "ridge_2.fit(X_train, Y_train)\n",
    "\n",
    "print(ridge_2.coef_)\n",
    "\n",
    "Y_pred_2s = ridge_2.predict(X_test)\n",
    "res = pd.DataFrame(data=np.concatenate([Y_pred_2s, Y_test, abs(Y_test - Y_pred_2s)], axis=1),\n",
    "                   columns=[\"Prediction\", \"Ground Truth\", \"Error\"])\n",
    "print(res.head(5))\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(Y_test, Y_pred_2s))\n",
    "print(f'RMSE = {rmse}')\n",
    "r2 = r2_score(Y_test, Y_pred_2s)\n",
    "print(f'R2 Score = {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning\n",
    "a = 0.05\n",
    "alphas = []\n",
    "rmse = []\n",
    "best_rmse = float('inf')\n",
    "best_alpha = a\n",
    "\n",
    "while a < 1:\n",
    "    rr = linear_model.Ridge(alpha=a)\n",
    "    rr.fit(X_train, Y_train)\n",
    "    pred = rr.predict(X_validation)\n",
    "    error_rmse = math.sqrt(mean_squared_error(Y_validation, pred))\n",
    "    alphas.append(a)\n",
    "    rmse.append(error_rmse)\n",
    "    \n",
    "    if error_rmse < best_rmse:\n",
    "        best_rmse = error_rmse\n",
    "        best_alpha = a\n",
    "    \n",
    "    a += 0.05\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alphas, rmse, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs alpha on Train and Validate set')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Test with best hyperparameter\n",
    "rr = linear_model.Lasso(alpha = best_alpha)\n",
    "rr.fit(X_train, Y_train)\n",
    "Y_pred_3t = rr.predict(X_test)\n",
    "rmse = math.sqrt(mean_squared_error(Y_test, Y_pred_3t))\n",
    "print('Error on Test set')\n",
    "print(f'RMSE = {rmse}')\n",
    "r2 = r2_score(Y_test, Y_pred_3t)\n",
    "print(f'R2 Score = {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. LASSO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = linear_model.Lasso(alpha=0.1)\n",
    "lasso_model.fit(X_train, Y_train)\n",
    "print(lasso_model.intercept_, lasso_model.coef_)\n",
    "\n",
    "Y_pred_3 = lasso_model.predict(X_test).reshape(-1, 1)\n",
    "res = pd.DataFrame(data=np.concatenate([Y_pred_3, Y_test, abs(Y_test - Y_pred_3)], axis=1),\n",
    "                   columns=[\"Prediction\", \"Ground Truth\", \"Error\"])\n",
    "\n",
    "print(res.head(5))\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(Y_test, Y_pred_3))\n",
    "print(f'RMSE = {rmse}')\n",
    "r2 = r2_score(Y_test, Y_pred_3)\n",
    "print(f'R2 Score = {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning\n",
    "a = 0.05\n",
    "alphas = []\n",
    "rmse = []\n",
    "best_rmse = float('inf')\n",
    "best_alpha = a\n",
    "\n",
    "while a < 1:\n",
    "    rr = linear_model.Lasso(alpha=a)\n",
    "    rr.fit(X_train, Y_train)\n",
    "    pred = rr.predict(X_validation)\n",
    "    error_rmse = math.sqrt(mean_squared_error(Y_validation, pred))\n",
    "    alphas.append(a)\n",
    "    rmse.append(error_rmse)\n",
    "    \n",
    "    if error_rmse < best_rmse:\n",
    "        best_rmse = error_rmse\n",
    "        best_alpha = a\n",
    "    \n",
    "    a += 0.05\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(alphas, rmse, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE vs alpha on Train and Validate set')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Test with best hyperparameter\n",
    "rr = linear_model.Lasso(alpha = best_alpha)\n",
    "rr.fit(X_train, Y_train)\n",
    "Y_pred_3t = rr.predict(X_test)\n",
    "rmse = math.sqrt(mean_squared_error(Y_test, Y_pred_3t))\n",
    "print('Error on Test set')\n",
    "print(f'RMSE = {rmse}')\n",
    "r2 = r2_score(Y_test, Y_pred_3t)\n",
    "print(f'R2 Score = {r2}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
